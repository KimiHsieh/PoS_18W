# Assignment 4: MPI Collectives and MPI-IO #
## Introduction ##
This ﬁnal assignment will focus on collective and parallel IO operations available in MPI libraries. There is a large set of collective operations deﬁned in the MPI standard, and their use can greatly simplify the development process of parallel applications, while providing better performance when compared to equivalent point-to-point communication patterns. The set of parallel IO operations allows MPI libraries to abstract the underlying parallel ﬁle system implementation in a generic and standard interface.

The use of MPI collectives provides several beneﬁts to MPI developers. It is recommended that developers use collectives in their applications as much as possible. A programmer is free to emulate any collective operation through the use of point-to-point communication, but this would result on more code to verify for correctness. Additionally, MPI collectives use well tuned distributed algorithms that reduce the amount of communication required to complete the operations. These algorithms are designed with scalability in mind; therefore, applications that rely on MPI collectives are generally easier to scale to large number of processes. Last but not least, MPI implementations will use hardware acceleration for these operations when available.

The MPI-IO interface allows MPI applications to access parallel ﬁle systems through a standard interface. There are several parallel ﬁle systems available and while they provide POSIX interfaces, they usually require the use of proprietary interfaces in order to access their more attractive features and performance. Having a higher level and standard interface simpliﬁes the development process and improves portability. Additionally, MPI can reduce and streamline accesses to the ﬁle system though optimizations such as data sieving and 2-phase IO.

In this assignment, students will continue to tweak the provided Gaussian Elimination code by transforming it to use MPI collective operations on identiﬁed communication patterns. Additionally, the initialization code will be updated to use MPI-IO to process the input matrix and vector, and output the solution vector in a distributed manner. In both of these main tasks, the aim is to improve the quality and portability of the code.
